"""
Time Series Forecasting com TinyTimeMixer (TTM) - IBM Granite

Este m√≥dulo implementa um modelo de previs√£o de s√©ries temporais utilizando o TinyTimeMixer
da IBM Granite, aplicado para previs√£o de vendas e faturamento por produto e UF.

Instala√ß√£o das depend√™ncias:
pip install "granite-tsfm[notebooks] @ git+https://github.com/ibm-granite/granite-tsfm.git@v0.2.22"
pip install transformers torch accelerate bitsandbytes pandas scikit-learn gdown peft

Autor: [Seu Nome]
Data: [Data Atual]
Vers√£o: 3.0
"""

import pandas as pd
import torch
import numpy as np
import math
import os
from concurrent.futures import ProcessPoolExecutor

# Importa√ß√µes espec√≠ficas da biblioteca Hugging Face e TSFM para treinamento
from transformers import (
    BitsAndBytesConfig,      # Configura√ß√£o para quantiza√ß√£o de bits
    TrainingArguments,       # Argumentos de treinamento do modelo
    Trainer,                 # Classe principal para treinamento
    EarlyStoppingCallback,   # Callback para parada antecipada
)
from torch.optim import AdamW                    # Otimizador AdamW
from torch.optim.lr_scheduler import OneCycleLR  # Scheduler de learning rate
from peft import LoraConfig, get_peft_model      # Para fine-tuning eficiente

# Componentes da biblioteca IBM Granite TSFM para processamento de s√©ries temporais
from tsfm_public import (
    TimeSeriesPreprocessor,      # Preprocessador de s√©ries temporais
    TinyTimeMixerForPrediction,  # Modelo TTM para previs√£o
    ForecastDFDataset,           # Dataset personalizado para previs√£o
    TrackingCallback,            # Callback para tracking de m√©tricas
)
from tsfm_public.toolkit.time_series_preprocessor import prepare_data_splits
from tsfm_public.models.tinytimemixer.configuration_tinytimemixer import TinyTimeMixerConfig

# Importa√ß√µes para avalia√ß√£o do modelo
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib
matplotlib.use('Agg')  # Backend sem interface gr√°fica
import matplotlib.pyplot as plt
import seaborn as sns

# =============================================================================
# CONFIGURA√á√ïES GLOBAIS
# =============================================================================

# Configura√ß√µes de paths flex√≠veis
DATA_PATH = os.getenv('DATA_PATH', './dados/db_tratado-w.csv')
OUTPUT_DIR = os.getenv('OUTPUT_DIR', './results_ttm_model')
SAVE_PATH = os.getenv('MODEL_SAVE_PATH', './final_ttm_model')

# =============================================================================
# FUN√á√ïES UTILIT√ÅRIAS
# =============================================================================

def validate_dataframe(df):
    """Valida estrutura do DataFrame."""
    required_columns = ['date', 'produto_cat', 'uf_cat', 'vendas', 'faturamento']
    missing = [col for col in required_columns if col not in df.columns]
    if missing:
        raise ValueError(f"Colunas obrigat√≥rias ausentes: {missing}")
    
    if df.empty:
        raise ValueError("DataFrame est√° vazio")
    
    # Verificar tipos de dados
    if not pd.api.types.is_datetime64_any_dtype(df['date']):
        raise ValueError("Coluna 'date' deve ser datetime")
    
    numeric_cols = ['vendas', 'faturamento']
    for col in numeric_cols:
        if not pd.api.types.is_numeric_dtype(df[col]):
            raise ValueError(f"Coluna '{col}' deve ser num√©rica")
    
    return True

def clear_gpu_cache():
    """Limpa cache da GPU se dispon√≠vel."""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()

def monitor_gpu_usage():
    """Monitora o uso de mem√≥ria GPU durante o treinamento."""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1e9
        cached = torch.cuda.memory_reserved() / 1e9
        print(f"GPU Memory: {allocated:.2f}GB allocated, {cached:.2f}GB cached")

# =============================================================================
# SE√á√ÉO 1: CARREGAMENTO E CONFIGURA√á√ÉO INICIAL DOS DADOS
# =============================================================================

def load_data():
    """
    Carrega os dados de vendas e faturamento do arquivo CSV.
    
    Se o arquivo n√£o for encontrado, cria um DataFrame de exemplo para demonstra√ß√£o
    com dados sint√©ticos de vendas e faturamento por produto e UF.
    
    Returns:
        pd.DataFrame: DataFrame com colunas date, produto_cat, uf_cat, vendas, faturamento
    """
    try:
        # Tenta carregar o arquivo de dados real
        df = pd.read_csv(DATA_PATH, parse_dates=["date"])
        print(f"Dados carregados do arquivo {DATA_PATH}")
        validate_dataframe(df)
        return df
    except FileNotFoundError:
        print(f"Erro: Arquivo {DATA_PATH} n√£o encontrado. Criando DataFrame de exemplo...")
        
        # Criar dados de exemplo para demonstra√ß√£o
        data_example = {
            'date': pd.to_datetime(['2023-01-01', '2023-01-08', '2023-01-15', '2023-01-22', '2023-01-29', '2023-02-05',
                                    '2023-02-12', '2023-02-19', '2023-02-26', '2023-03-05', '2023-03-12', '2023-03-19']),
            'produto_cat': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],  # Categoria do produto (codificada)
            'uf_cat': [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],        # UF categorizada (codificada)
            'vendas': [300, 310, 305, 320, 315, 330, 325, 340, 335, 350, 345, 360],      # Volume de vendas
            'faturamento': [1000.0, 1100.0, 1050.0, 1200.0, 1150.0, 1300.0, 1250.0,     # Valor do faturamento
                           1400.0, 1350.0, 1500.0, 1450.0, 1600.0]
        }
        df_single_product = pd.DataFrame(data_example)
        
        # Simular m√∫ltiplos produtos (1-3) e UFs (1-4) para dataset otimizado para GTX 1650
        df = pd.concat([df_single_product.copy().assign(produto_cat=i, uf_cat=j)
                        for i in range(1, 4) for j in range(1, 5)], ignore_index=True)
        df['date'] = pd.to_datetime(df['date'])
        print("DataFrame de exemplo criado com 3 produtos x 4 UFs = 12 combina√ß√µes")
        validate_dataframe(df)
        return df
    except Exception as e:
        print(f"Erro ao carregar dados: {e}")
        raise

# Carregar dados
df = load_data()

print("\nInforma√ß√µes do DataFrame carregado:")
print(df.info())
print("\nPrimeiras linhas do DataFrame:")
print(df.head())

# =============================================================================
# CONFIGURA√á√ïES PRINCIPAIS DO MODELO E DATASET
# =============================================================================

# Configura√ß√µes das colunas do dataset
TIMESTAMP_COLUMN = 'date'                          # Coluna de timestamp (data)
ID_COLUMNS = ['produto_cat', 'uf_cat']             # Colunas identificadoras das s√©ries
TARGET_COLUMNS = ['vendas', 'faturamento']         # Vari√°veis alvo para previs√£o
STATIC_CATEGORICAL_COLUMNS = ['uf_cat']            # Vari√°veis categ√≥ricas est√°ticas
CONTROL_COLUMNS = []                               # Vari√°veis de controle (vazio neste caso)

# Configura√ß√µes temporais do modelo - mantendo contexto original para melhor performance
CONTEXT_LENGTH = 104    # Janela de contexto: 2 anos de hist√≥rico semanal (104 semanas)
PREDICTION_LENGTH = 26  # Horizonte de previs√£o: 6 meses semanais (26 semanas)

# Configura√ß√£o autom√°tica do dispositivo de processamento
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(f"\nDispositivo de processamento: {DEVICE}")
if DEVICE == "cuda":
    print(f"GPU dispon√≠vel: {torch.cuda.get_device_name(0)}")
    print(f"Mem√≥ria GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

# =============================================================================
# SE√á√ÉO 2: PR√â-PROCESSAMENTO E PREPARA√á√ÉO DOS DADOS
# =============================================================================

def process_single_group(args):
    """
    Processa um √∫nico grupo para resampling paralelo.
    
    Args:
        args (tuple): (group_df, timestamp_col, id_cols, group_keys)
    
    Returns:
        pd.DataFrame: Grupo processado com resampling semanal
    """
    group_df, timestamp_col, id_cols, group_keys = args
    
    try:
        # Definir timestamp como √≠ndice para resampling
        group_df = group_df.set_index(timestamp_col)
        
        # Aplicar resampling semanal (W) pegando √∫ltimo valor de cada semana
        group_df = group_df.resample('W').last().ffill()
        
        # Resetar √≠ndice para voltar timestamp como coluna
        group_df = group_df.reset_index()
        
        # Restaurar colunas identificadoras (produto_cat, uf_cat)
        for i, col in enumerate(id_cols):
            group_df[col] = group_keys[i] if isinstance(group_keys, tuple) else group_keys
        
        return group_df
    except Exception as e:
        print(f"Erro processando grupo {group_keys}: {e}")
        return pd.DataFrame()

def resample_data_to_weekly(df, timestamp_col, id_cols):
    """
    Resampling dos dados para frequ√™ncia semanal consistente usando processamento paralelo.
    
    Args:
        df (pd.DataFrame): DataFrame original
        timestamp_col (str): Nome da coluna de timestamp
        id_cols (list): Lista das colunas identificadoras
    
    Returns:
        pd.DataFrame: DataFrame com frequ√™ncia semanal consistente
    """
    print("Aplicando resampling paralelo para frequ√™ncia semanal...")
    
    # Ordenar dados por timestamp e IDs para consist√™ncia
    df = df.sort_values([timestamp_col] + id_cols).reset_index(drop=True)
    
    # Preparar argumentos para processamento paralelo
    group_args = []
    for group_keys, group_df in df.groupby(id_cols):
        group_args.append((group_df.copy(), timestamp_col, id_cols, group_keys))
    
    total_groups = len(group_args)
    print(f"Processando {total_groups} grupos produto-UF em paralelo...")
    
    # Processar grupos em paralelo usando 4 workers
    with ProcessPoolExecutor(max_workers=4) as executor:
        df_resampled = list(executor.map(process_single_group, group_args))
    
    # Filtrar grupos vazios e concatenar
    df_resampled = [df for df in df_resampled if not df.empty]
    
    if not df_resampled:
        raise ValueError("Nenhum grupo foi processado com sucesso")
    
    print(f"Resampling paralelo conclu√≠do.")
    return pd.concat(df_resampled, ignore_index=True)

# Aplicar resampling nos dados
df = resample_data_to_weekly(df, TIMESTAMP_COLUMN, ID_COLUMNS)
print(f"Resampling conclu√≠do. Dataset final: {len(df)} registros")

def create_time_series_preprocessor():
    """
    Cria e configura o preprocessador de s√©ries temporais TTM.
    
    Returns:
        TimeSeriesPreprocessor: Preprocessador configurado para o dataset
    """
    print("Configurando preprocessador de s√©ries temporais...")
    
    return TimeSeriesPreprocessor(
        id_columns=ID_COLUMNS,                      # Colunas identificadoras das s√©ries
        timestamp_column=TIMESTAMP_COLUMN,          # Coluna de timestamp
        target_columns=TARGET_COLUMNS,              # Vari√°veis alvo para previs√£o
        static_categorical_columns=STATIC_CATEGORICAL_COLUMNS,  # Vari√°veis categ√≥ricas
        scaling_id_columns=ID_COLUMNS,              # Colunas para escalonamento por grupo
        context_length=CONTEXT_LENGTH,              # Tamanho da janela de contexto
        prediction_length=PREDICTION_LENGTH,        # Horizonte de previs√£o
        scaling=True,                              # Aplicar normaliza√ß√£o/escalonamento
        scaler_type="standard",                    # Tipo de escalonamento (StandardScaler)
        encode_categorical=True,                   # Codificar vari√°veis categ√≥ricas
        control_columns=CONTROL_COLUMNS,           # Vari√°veis de controle externas
        observable_columns=[],                     # Vari√°veis observ√°veis durante previs√£o
        freq='W'                                   # Frequ√™ncia dos dados (semanal)
    )

# Criar e treinar preprocessador
tsp = create_time_series_preprocessor()
print("Treinando preprocessador com todos os dados...")
trained_tsp = tsp.train(df)

# Aplicar preprocessamento aos dados
print("Aplicando transforma√ß√µes de preprocessamento...")
df_processed = trained_tsp.preprocess(df)
print(f"Preprocessamento conclu√≠do. Shape dos dados processados: {df_processed.shape}")

# =============================================================================
# SE√á√ÉO 3: DIVIS√ÉO DOS DADOS E CRIA√á√ÉO DOS DATASETS
# =============================================================================

def split_data_by_combinations(df_processed, id_columns, train_frac=0.7, val_frac=0.15, random_state=42):
    """
    Divide os dados preservando combina√ß√µes produto-UF inteiras.
    
    Evita vazamento de dados garantindo que uma combina√ß√£o produto-UF
    n√£o apare√ßa em m√∫ltiplos conjuntos (treino/valida√ß√£o/teste).
    
    Args:
        df_processed (pd.DataFrame): Dados preprocessados
        id_columns (list): Colunas identificadoras para preservar
        train_frac (float): Fra√ß√£o para treinamento (padr√£o: 0.7)
        val_frac (float): Fra√ß√£o para valida√ß√£o (padr√£o: 0.15)
        random_state (int): Seed para reprodutibilidade
    
    Returns:
        tuple: (train_data, valid_data, test_data)
    """
    print("Dividindo dados preservando combina√ß√µes produto-UF...")
    
    # Obter combina√ß√µes √∫nicas de produto-UF
    unique_combinations = df_processed[id_columns].drop_duplicates()
    total_combinations = len(unique_combinations)
    
    if total_combinations < 3:
        raise ValueError(f"Muito poucas combina√ß√µes ({total_combinations}) para divis√£o treino/val/teste")
    
    # Divis√£o estratificada das combina√ß√µes
    train_combinations = unique_combinations.sample(frac=train_frac, random_state=random_state)
    remaining = unique_combinations.drop(train_combinations.index)
    
    # Da parte restante, dividir entre valida√ß√£o e teste
    val_frac_remaining = val_frac / (1 - train_frac)  # Ajustar fra√ß√£o para o restante
    valid_combinations = remaining.sample(frac=val_frac_remaining, random_state=random_state)
    test_combinations = remaining.drop(valid_combinations.index)
    
    # Filtrar dados processados baseado nas combina√ß√µes
    train_data = df_processed.merge(train_combinations, on=id_columns)
    valid_data = df_processed.merge(valid_combinations, on=id_columns)
    test_data = df_processed.merge(test_combinations, on=id_columns)
    
    print(f"Divis√£o conclu√≠da:")
    print(f"  Treino: {len(train_combinations):>3} combina√ß√µes ({len(train_data):>4} registros)")
    print(f"  Valida√ß√£o: {len(valid_combinations):>3} combina√ß√µes ({len(valid_data):>4} registros)")
    print(f"  Teste: {len(test_combinations):>3} combina√ß√µes ({len(test_data):>4} registros)")
    print(f"  Total: {total_combinations} combina√ß√µes √∫nicas")
    
    return train_data, valid_data, test_data

# Aplicar divis√£o dos dados
train_data, valid_data, test_data = split_data_by_combinations(df_processed, ID_COLUMNS)

def create_forecast_datasets(train_data, valid_data, test_data, trained_tsp):
    """
    Cria os datasets espec√≠ficos para o modelo TTM de previs√£o.
    
    Args:
        train_data, valid_data, test_data (pd.DataFrame): Dados divididos
        trained_tsp (TimeSeriesPreprocessor): Preprocessador treinado
    
    Returns:
        tuple: (train_dataset, valid_dataset, test_dataset)
    """
    print("Criando datasets TTM para treinamento...")
    
    # Obter frequency token do preprocessador
    freq_token = trained_tsp.get_frequency_token(trained_tsp.freq)
    print(f"Frequency token: {freq_token}")
    
    # Configura√ß√£o comum para todos os datasets
    dataset_config = {
        'id_columns': ID_COLUMNS,
        'timestamp_column': TIMESTAMP_COLUMN,
        'target_columns': TARGET_COLUMNS,
        'control_columns': CONTROL_COLUMNS,
        'static_categorical_columns': STATIC_CATEGORICAL_COLUMNS,
        'context_length': CONTEXT_LENGTH,
        'prediction_length': PREDICTION_LENGTH,
        'frequency_token': freq_token
    }
    
    # Criar datasets para cada divis√£o
    train_dataset = ForecastDFDataset(train_data, **dataset_config)
    valid_dataset = ForecastDFDataset(valid_data, **dataset_config)
    test_dataset = ForecastDFDataset(test_data, **dataset_config)
    
    print(f"Datasets TTM criados:")
    print(f"  Treino: {len(train_dataset):>4} amostras")
    print(f"  Valida√ß√£o: {len(valid_dataset):>4} amostras")
    print(f"  Teste: {len(test_dataset):>4} amostras")
    
    return train_dataset, valid_dataset, test_dataset

# Criar datasets TTM
train_dataset, valid_dataset, test_dataset = create_forecast_datasets(
    train_data, valid_data, test_data, trained_tsp
)

# =============================================================================
# SE√á√ÉO 4: CONFIGURA√á√ÉO E INICIALIZA√á√ÉO DO MODELO TTM
# =============================================================================

def create_ttm_config(trained_tsp):
    """
    Cria configura√ß√£o personalizada para o modelo TinyTimeMixer.
    
    Adapta o modelo pr√©-treinado para o dataset espec√≠fico,
    configurando canais de entrada, sa√≠da e vari√°veis categ√≥ricas.
    
    Args:
        trained_tsp (TimeSeriesPreprocessor): Preprocessador treinado
    
    Returns:
        TinyTimeMixerConfig: Configura√ß√£o do modelo TTM
    """
    print("Configurando modelo TinyTimeMixer...")
    
    config = TinyTimeMixerConfig(
        context_length=CONTEXT_LENGTH,                      # Janela de contexto hist√≥rico
        prediction_length=PREDICTION_LENGTH,                # Horizonte de previs√£o
        num_input_channels=trained_tsp.num_input_channels,  # N√∫mero de canais de entrada
        prediction_channel_indices=trained_tsp.prediction_channel_indices,  # √çndices dos canais alvo
        exogenous_channel_indices=trained_tsp.exogenous_channel_indices,    # √çndices de vari√°veis ex√≥genas
        decoder_mode="mix_channel",                         # Modo de decodifica√ß√£o (mix de canais)
        categorical_vocab_size_list=trained_tsp.categorical_vocab_size_list,  # Tamanhos dos vocabul√°rios categ√≥ricos
    )
    
    print(f"Configura√ß√£o TTM:")
    print(f"  Context Length: {config.context_length}")
    print(f"  Prediction Length: {config.prediction_length}")
    print(f"  Input Channels: {config.num_input_channels}")
    print(f"  Prediction Channels: {len(config.prediction_channel_indices)}")
    print(f"  Categorical Vocabularies: {config.categorical_vocab_size_list}")
    
    return config

def load_pretrained_ttm_model(config):
    """
    Carrega modelo TTM pr√©-treinado da IBM Granite para GTX 1650.
    
    Utiliza configura√ß√µes de mem√≥ria otimizadas mantendo FP32 para estabilidade.
    
    Args:
        config (TinyTimeMixerConfig): Configura√ß√£o do modelo
    
    Returns:
        TinyTimeMixerForPrediction: Modelo TTM carregado
    """
    print("Carregando modelo TTM pr√©-treinado...")
    
    # Limpar cache antes de carregar modelo
    clear_gpu_cache()
    
    model = TinyTimeMixerForPrediction.from_pretrained(
        "ibm-granite/granite-timeseries-ttm-r2",    # Modelo base pr√©-treinado
        config=config,                              # Configura√ß√£o personalizada
        device_map="auto",                          # Mapeamento autom√°tico de dispositivos
        ignore_mismatched_sizes=True,              # Ignorar incompatibilidades de tamanho
        low_cpu_mem_usage=True,                    # Reduzir uso de RAM
    )
    
    print(f"Modelo carregado: {model.__class__.__name__}")
    monitor_gpu_usage()
    return model

def setup_selective_fine_tuning(model):
    """
    Configura fine-tuning seletivo congelando a maioria das camadas.
    
    Mant√©m apenas as camadas fully-connected (fc1/fc2) trein√°veis,
    reduzindo significativamente o n√∫mero de par√¢metros a treinar.
    
    Args:
        model: Modelo TTM carregado
    
    Returns:
        None (modifica modelo in-place)
    """
    print("Configurando fine-tuning seletivo...")
    
    # Congelar todas as camadas inicialmente
    for param in model.parameters():
        param.requires_grad = False
    
    # Descongelar apenas camadas fc1 e fc2 (fully-connected)
    trainable_layers = []
    for name, module in model.named_modules():
        if 'fc1' in name or 'fc2' in name:
            if isinstance(module, torch.nn.Linear):
                module.requires_grad_(True)
                trainable_layers.append(name)
    
    # Calcular estat√≠sticas de par√¢metros
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())
    trainable_percentage = (trainable_params / total_params) * 100 if total_params > 0 else 0
    
    print(f"Fine-tuning seletivo configurado:")
    print(f"  Camadas trein√°veis: {trainable_layers}")
    print(f"  Par√¢metros trein√°veis: {trainable_params:,} / {total_params:,} ({trainable_percentage:.1f}%)")
    print(f"  Redu√ß√£o de par√¢metros: {100-trainable_percentage:.1f}%")

# Executar configura√ß√£o do modelo
model_config = create_ttm_config(trained_tsp)
model = load_pretrained_ttm_model(model_config)
setup_selective_fine_tuning(model)

# =============================================================================
# SE√á√ÉO 5: CONFIGURA√á√ÉO E EXECU√á√ÉO DO TREINAMENTO
# =============================================================================

# Hiperpar√¢metros de treinamento otimizados para GTX 1650 4GB
LEARNING_RATE = 5e-4                # Taxa de aprendizado otimizada para fine-tuning
NUM_TRAIN_EPOCHS = 100              # N√∫mero m√°ximo de √©pocas (com early stopping)
PER_DEVICE_TRAIN_BATCH_SIZE = 4     # Batch size ajustado para FP32 na GTX 1650
PER_DEVICE_EVAL_BATCH_SIZE = 8      # Batch size valida√ß√£o ajustado

def create_training_arguments():
    """
    Cria argumentos de treinamento otimizados para GTX 1650 4GB.
    
    Utiliza FP32, workers paralelos e gradient accumulation para 
    estabilidade e performance na GTX 1650.
    
    Returns:
        TrainingArguments: Configura√ß√£o de treinamento
    """
    return TrainingArguments(
        output_dir=OUTPUT_DIR,                         # Diret√≥rio de sa√≠da
        overwrite_output_dir=True,                     # Sobrescrever resultados anteriores
        learning_rate=LEARNING_RATE,                   # Taxa de aprendizado
        num_train_epochs=NUM_TRAIN_EPOCHS,             # N√∫mero de √©pocas
        do_eval=True,                                  # Executar avalia√ß√£o
        eval_strategy="epoch",                         # Avaliar a cada √©poca
        per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,  # Batch size treino
        per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH_SIZE,    # Batch size valida√ß√£o
        dataloader_num_workers=4,                      # Workers paralelos
        dataloader_pin_memory=True,                    # Pin memory para transfer√™ncia GPU mais r√°pida
        gradient_accumulation_steps=4,                 # Accumular gradientes para simular batch maior
        fp16=False,                                    # FP32 para estabilidade
        bf16=False,                                    # BF16 desabilitado
        max_grad_norm=1.0,                            # Gradient clipping para estabilidade
        report_to="none",                              # N√£o reportar para ferramentas externas
        save_strategy="epoch",                         # Salvar modelo a cada √©poca
        logging_strategy="epoch",                      # Log de m√©tricas a cada √©poca
        save_total_limit=2,                           # Manter apenas 2 checkpoints
        logging_dir=f"{OUTPUT_DIR}/logs",             # Diret√≥rio de logs
        load_best_model_at_end=True,                  # Carregar melhor modelo ao final
        metric_for_best_model="eval_loss",            # M√©trica para sele√ß√£o do melhor modelo
        greater_is_better=False,                      # Menor loss √© melhor
        use_cpu=DEVICE == "cpu",                      # For√ßar CPU se necess√°rio
    )

def create_training_callbacks():
    """
    Cria callbacks para controle avan√ßado do treinamento.
    
    Inclui early stopping para evitar overfitting e tracking
    personalizado de m√©tricas durante o treinamento.
    
    Returns:
        list: Lista de callbacks configurados
    """
    # Early stopping: para quando n√£o h√° melhoria por 15 √©pocas
    early_stopping_callback = EarlyStoppingCallback(
        early_stopping_patience=15,     # Paci√™ncia: 15 √©pocas sem melhoria
        early_stopping_threshold=0.0,   # Threshold m√≠nimo para considerar melhoria
    )
    
    # Callback para tracking personalizado de m√©tricas
    tracking_callback = TrackingCallback()
    
    return [early_stopping_callback, tracking_callback]

def create_optimizer_and_scheduler(model, train_dataset_size):
    """
    Cria otimizador e scheduler de learning rate otimizados.
    
    Utiliza AdamW com OneCycleLR para converg√™ncia r√°pida e est√°vel.
    
    Args:
        model: Modelo para otimiza√ß√£o
        train_dataset_size (int): Tamanho do dataset de treino
    
    Returns:
        tuple: (optimizer, scheduler)
    """
    # Otimizador AdamW (vers√£o melhorada do Adam)
    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)
    
    # Scheduler OneCycleLR para varia√ß√£o c√≠clica do learning rate
    steps_per_epoch = math.ceil(train_dataset_size / (PER_DEVICE_TRAIN_BATCH_SIZE * 4))  # Considerando gradient accumulation
    scheduler = OneCycleLR(
        optimizer,
        max_lr=LEARNING_RATE,                # Learning rate m√°ximo
        epochs=NUM_TRAIN_EPOCHS,             # Total de √©pocas
        steps_per_epoch=steps_per_epoch,     # Steps por √©poca
    )
    
    print(f"Otimiza√ß√£o configurada:")
    print(f"  Otimizador: AdamW (lr={LEARNING_RATE})")
    print(f"  Scheduler: OneCycleLR")
    print(f"  Steps por √©poca: {steps_per_epoch}")
    
    return optimizer, scheduler

class TTMTrainer(Trainer):
    """
    Trainer customizado para o modelo TinyTimeMixer.
    
    Sobrescreve o m√©todo compute_loss para filtrar adequadamente
    as entradas compat√≠veis com o modelo TTM.
    """
    
    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        """
        Calcula a fun√ß√£o de perda personalizada para TTM.
        
        Filtra apenas as chaves de entrada v√°lidas para o modelo TTM,
        evitando erros de entrada incompat√≠vel.
        
        Args:
            model: Modelo TTM
            inputs (dict): Inputs do batch
            return_outputs (bool): Se deve retornar outputs al√©m da loss
            num_items_in_batch: N√∫mero de itens no batch
        
        Returns:
            torch.Tensor ou tuple: Loss (e outputs se solicitado)
        """
        # Chaves v√°lidas aceitas pelo modelo TTM
        valid_keys = [
            'past_values',              # Valores hist√≥ricos
            'future_values',            # Valores futuros (targets)
            'past_observed_mask',       # M√°scara de valores observados no passado
            'future_observed_mask',     # M√°scara de valores observados no futuro
            'freq_token',               # Token de frequ√™ncia temporal
            'static_categorical_values' # Valores categ√≥ricos est√°ticos
        ]
        
        # Filtrar apenas entradas v√°lidas
        filtered_inputs = {k: v for k, v in inputs.items() if k in valid_keys}
        
        try:
            # Forward pass no modelo
            outputs = model(**filtered_inputs)
            
            # Extrair loss dos outputs
            loss = outputs.loss if hasattr(outputs, 'loss') else outputs[0]
            
            return (loss, outputs) if return_outputs else loss
        except Exception as e:
            print(f"Erro durante compute_loss: {e}")
            # Retornar loss zero em caso de erro
            dummy_loss = torch.tensor(0.0, requires_grad=True, device=model.device)
            return dummy_loss

def train_ttm_model():
    """
    Executa o treinamento completo do modelo TTM.
    
    Configura todos os componentes necess√°rios e executa o treinamento
    com monitoramento e salvamento autom√°tico do melhor modelo.
    
    Returns:
        Trainer: Trainer ap√≥s conclus√£o do treinamento
    """
    print("\n" + "="*80)
    print("INICIANDO TREINAMENTO DO MODELO TTM")
    print("="*80)
    
    # Monitorar uso inicial da GPU
    monitor_gpu_usage()
    
    # Criar componentes de treinamento
    training_args = create_training_arguments()
    callbacks = create_training_callbacks()
    optimizer, scheduler = create_optimizer_and_scheduler(model, len(train_dataset))
    
    # Inicializar trainer customizado
    trainer = TTMTrainer(
        model=model,                        # Modelo TTM configurado
        args=training_args,                 # Argumentos de treinamento
        train_dataset=train_dataset,        # Dataset de treino
        eval_dataset=valid_dataset,         # Dataset de valida√ß√£o
        callbacks=callbacks,                # Callbacks (early stopping, tracking)
        optimizers=(optimizer, scheduler),  # Otimizador e scheduler
    )
    
    print(f"Trainer configurado:")
    print(f"  Epochs m√°ximas: {NUM_TRAIN_EPOCHS}")
    print(f"  Batch size treino: {PER_DEVICE_TRAIN_BATCH_SIZE}")
    print(f"  Batch size valida√ß√£o: {PER_DEVICE_EVAL_BATCH_SIZE}")
    print(f"  Gradient accumulation: 4 steps")
    print(f"  Precision: FP32")
    print(f"  Workers paralelos: 4")
    print(f"  Early stopping: 15 √©pocas de paci√™ncia")
    print(f"  Dispositivo: {DEVICE}")
    
    print("\nIniciando treinamento...")
    
    try:
        # Executar treinamento
        trainer.train()
        
        print("\n" + "="*80)
        print("TREINAMENTO CONCLU√çDO")
        print("="*80)
    except Exception as e:
        print(f"Erro durante treinamento: {e}")
        raise
    finally:
        # Limpar cache independentemente do resultado
        clear_gpu_cache()
        monitor_gpu_usage()
    
    return trainer

def save_final_model(trainer, save_path=None):
    """
    Salva o modelo final treinado.
    
    Args:
        trainer: Trainer ap√≥s treinamento
        save_path (str): Caminho para salvar o modelo
    """
    if save_path is None:
        save_path = SAVE_PATH
    
    print(f"\nSalvando modelo final em: {save_path}")
    
    # Criar diret√≥rio se n√£o existir
    os.makedirs(save_path, exist_ok=True)
    
    try:
        trainer.save_model(save_path)
        
        # Salvar tamb√©m informa√ß√µes do preprocessador
        import pickle
        preprocessor_path = f"{save_path}/preprocessor.pkl"
        with open(preprocessor_path, 'wb') as f:
            pickle.dump(trained_tsp, f)
        
        print(f"Modelo salvo com sucesso!")
        print(f"  Modelo: {save_path}")
        print(f"  Preprocessador: {preprocessor_path}")
    except Exception as e:
        print(f"Erro ao salvar modelo: {e}")
        raise

# =============================================================================
# SE√á√ÉO 6: AVALIA√á√ÉO DO MODELO
# =============================================================================

def calculate_metrics(y_true, y_pred):
    """Calcula m√©tricas de avalia√ß√£o para s√©ries temporais."""
    # Remover NaNs e infinitos
    mask = ~(np.isnan(y_true) | np.isnan(y_pred) | np.isinf(y_true) | np.isinf(y_pred))
    y_true_clean = y_true[mask]
    y_pred_clean = y_pred[mask]
    
    if len(y_true_clean) == 0:
        return {'MAE': np.nan, 'RMSE': np.nan, 'MAPE': np.nan, 'R2': np.nan}
    
    try:
        mae = mean_absolute_error(y_true_clean, y_pred_clean)
        rmse = np.sqrt(mean_squared_error(y_true_clean, y_pred_clean))
        mape = np.mean(np.abs((y_true_clean - y_pred_clean) / (np.abs(y_true_clean) + 1e-8))) * 100
        r2 = r2_score(y_true_clean, y_pred_clean)
        
        return {'MAE': mae, 'RMSE': rmse, 'MAPE': mape, 'R2': r2}
    except Exception as e:
        print(f"Erro calculando m√©tricas: {e}")
        return {'MAE': np.nan, 'RMSE': np.nan, 'MAPE': np.nan, 'R2': np.nan}

def evaluate_model(trainer, test_dataset):
    """Avalia o modelo no conjunto de teste."""
    print("\n" + "="*60)
    print("AVALIA√á√ÉO DO MODELO")
    print("="*60)
    
    try:
        predictions = trainer.predict(test_dataset)
        y_pred = predictions.predictions
        y_true = predictions.label_ids
        
        print(f"Shape das predi√ß√µes: {y_pred.shape}")
        
        # M√©tricas por vari√°vel
        results = {}
        for i, target_name in enumerate(TARGET_COLUMNS):
            y_true_target = y_true[:, :, i].flatten()
            y_pred_target = y_pred[:, :, i].flatten()
            
            metrics = calculate_metrics(y_true_target, y_pred_target)
            results[target_name] = metrics
            
            print(f"\n{target_name.upper()}:")
            print(f"  MAE: {metrics['MAE']:.4f}")
            print(f"  RMSE: {metrics['RMSE']:.4f}")
            print(f"  MAPE: {metrics['MAPE']:.2f}%")
            print(f"  R¬≤: {metrics['R2']:.4f}")
        
        # M√©tricas gerais
        overall_metrics = calculate_metrics(y_true.flatten(), y_pred.flatten())
        results['overall'] = overall_metrics
        
        print(f"\nGERAL:")
        print(f"  MAE: {overall_metrics['MAE']:.4f}")
        print(f"  RMSE: {overall_metrics['RMSE']:.4f}")
        print(f"  MAPE: {overall_metrics['MAPE']:.2f}%")
        print(f"  R¬≤: {overall_metrics['R2']:.4f}")
        
        return results, y_true, y_pred
    except Exception as e:
        print(f"Erro durante avalia√ß√£o: {e}")
        raise

def create_evaluation_plots(y_true, y_pred, save_path="./evaluation_plots"):
    """Cria gr√°ficos essenciais de avalia√ß√£o."""
    os.makedirs(save_path, exist_ok=True)
    
    for i, target_name in enumerate(TARGET_COLUMNS):
        try:
            y_true_target = y_true[:, :, i].flatten()
            y_pred_target = y_pred[:, :, i].flatten()
            
            # Remover NaNs e infinitos
            mask = ~(np.isnan(y_true_target) | np.isnan(y_pred_target) | 
                    np.isinf(y_true_target) | np.isinf(y_pred_target))
            y_true_clean = y_true_target[mask]
            y_pred_clean = y_pred_target[mask]
            
            if len(y_true_clean) == 0:
                continue
            
            # Gr√°fico: Predito vs Real
            plt.figure(figsize=(8, 6))
            plt.scatter(y_true_clean, y_pred_clean, alpha=0.6)
            min_val = min(y_true_clean.min(), y_pred_clean.min())
            max_val = max(y_true_clean.max(), y_pred_clean.max())
            plt.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8)
            plt.xlabel('Valores Reais')
            plt.ylabel('Valores Preditos')
            plt.title(f'{target_name} - Predito vs Real')
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            plt.savefig(f"{save_path}/{target_name}_evaluation.png", dpi=150, bbox_inches='tight')
            plt.close()
        except Exception as e:
            print(f"Erro criando gr√°fico para {target_name}: {e}")

def save_evaluation_report(results, save_path="./evaluation_report.txt"):
    """Salva relat√≥rio conciso de avalia√ß√£o."""
    try:
        with open(save_path, 'w') as f:
            f.write("RELAT√ìRIO DE AVALIA√á√ÉO - MODELO TTM\n")
            f.write("="*50 + "\n\n")
            
            # Por vari√°vel
            for target_name, metrics in results.items():
                if target_name == 'overall':
                    continue
                f.write(f"{target_name.upper()}:\n")
                f.write(f"  MAE: {metrics['MAE']:.4f}\n")
                f.write(f"  RMSE: {metrics['RMSE']:.4f}\n")
                f.write(f"  MAPE: {metrics['MAPE']:.2f}%\n")
                f.write(f"  R¬≤: {metrics['R2']:.4f}\n\n")
            
            # Geral
            overall = results['overall']
            f.write("M√âTRICAS GERAIS:\n")
            f.write(f"  MAE: {overall['MAE']:.4f}\n")
            f.write(f"  RMSE: {overall['RMSE']:.4f}\n")
            f.write(f"  MAPE: {overall['MAPE']:.2f}%\n")
            f.write(f"  R¬≤: {overall['R2']:.4f}\n")
    except Exception as e:
        print(f"Erro salvando relat√≥rio: {e}")

def run_evaluation(trainer, test_dataset):
    """Executa avalia√ß√£o completa."""
    try:
        results, y_true, y_pred = evaluate_model(trainer, test_dataset)
        create_evaluation_plots(y_true, y_pred)
        save_evaluation_report(results)
        
        print(f"\n‚úÖ Avalia√ß√£o conclu√≠da!")
        print(f"üìä Gr√°ficos: ./evaluation_plots/")
        print(f"üìÑ Relat√≥rio: ./evaluation_report.txt")
        
        return results
    except Exception as e:
        print(f"Erro durante avalia√ß√£o: {e}")
        return {}

# =============================================================================
# EXECU√á√ÉO DO TREINAMENTO E AVALIA√á√ÉO
# =============================================================================

if __name__ == "__main__":
    try:
        trainer = train_ttm_model()
        evaluation_results = run_evaluation(trainer, test_dataset)
        save_final_model(trainer)
        
        print(f"\nüéâ Pipeline conclu√≠do!")
        if evaluation_results and 'overall' in evaluation_results:
            print(f"üìà R¬≤ geral: {evaluation_results['overall']['R2']:.4f}")
            print(f"üìâ MAE geral: {evaluation_results['overall']['MAE']:.4f}")
    except Exception as e:
        print(f"‚ùå Erro no pipeline: {e}")
        raise

# =================== C√ìDIGO TEMPOR√ÅRIO - INFER√äNCIA AUTOMATIZADA ===================
# Controle on/off
EXECUTAR_INFERENCIA_AUTOMATIZADA = False  # Mude para True para executar

if EXECUTAR_INFERENCIA_AUTOMATIZADA:
    import pandas as pd
    import numpy as np
    from itertools import product
    import torch
    
    def gerar_previsoes_automatizadas(trainer_modelo, preprocessador_treinado, 
                                    lista_produtos, lista_estados, 
                                    n_semanas=26, arquivo_saida='previsoes_automatizadas.csv'):
        """
        Gera previs√µes automatizadas usando o modelo TTM treinado
        """
        print(f"Gerando previs√µes para {len(lista_produtos)} produtos x {len(lista_estados)} estados x {n_semanas} semanas...")
        
        resultados = []
        
        # Obter estat√≠sticas dos dados originais para gerar contexto realista
        base_vendas = df['vendas'].mean() if len(df) > 0 else 300
        std_vendas = df['vendas'].std() if len(df) > 0 else 30
        base_faturamento = df['faturamento'].mean() if len(df) > 0 else 5000
        std_faturamento = df['faturamento'].std() if len(df) > 0 else 500
        
        # Data base para come√ßar as previs√µes
        data_inicio = pd.to_datetime('2024-01-01')  # Ajuste conforme necess√°rio
        
        for produto in lista_produtos:
            for estado in lista_estados:
                try:
                    # Criar dados hist√≥ricos realistas para o contexto (104 semanas)
                    dates = pd.date_range(start=data_inicio - pd.Timedelta(weeks=CONTEXT_LENGTH), 
                                         periods=CONTEXT_LENGTH, freq='W')
                    
                    # Dados hist√≥ricos com tend√™ncia e sazonalidade b√°sica
                    trend = np.linspace(0.9, 1.1, CONTEXT_LENGTH)  # Tend√™ncia leve
                    seasonal = 1 + 0.1 * np.sin(np.arange(CONTEXT_LENGTH) * 2 * np.pi / 52)  # Sazonalidade anual
                    
                    vendas_hist = np.maximum(0, base_vendas * trend * seasonal + 
                                           np.random.normal(0, std_vendas * 0.2, CONTEXT_LENGTH))
                    faturamento_hist = np.maximum(0, base_faturamento * trend * seasonal + 
                                                 np.random.normal(0, std_faturamento * 0.2, CONTEXT_LENGTH))
                    
                    hist_data = pd.DataFrame({
                        TIMESTAMP_COLUMN: dates,
                        'produto_cat': [produto] * CONTEXT_LENGTH,
                        'uf_cat': [estado] * CONTEXT_LENGTH,
                        'vendas': vendas_hist,
                        'faturamento': faturamento_hist
                    })
                    
                    # Preprocessar dados hist√≥ricos
                    hist_processed = preprocessador_treinado.preprocess(hist_data)
                    
                    # Criar dataset para predi√ß√£o
                    pred_dataset = ForecastDFDataset(
                        hist_processed,
                        id_columns=ID_COLUMNS,
                        timestamp_column=TIMESTAMP_COLUMN,
                        target_columns=TARGET_COLUMNS,
                        context_length=CONTEXT_LENGTH,
                        prediction_length=PREDICTION_LENGTH,
                        frequency_token=preprocessador_treinado.get_frequency_token('W'),
                        static_categorical_columns=STATIC_CATEGORICAL_COLUMNS,
                        control_columns=CONTROL_COLUMNS
                    )
                    
                    # Fazer predi√ß√£o usando o trainer
                    if len(pred_dataset) > 0:
                        predictions = trainer_modelo.predict(pred_dataset)
                        pred_values = predictions.predictions[0]  # Primeira (e √∫nica) previs√£o
                        
                        # Extrair previs√µes para cada semana
                        for semana in range(1, min(n_semanas + 1, PREDICTION_LENGTH + 1)):
                            vendas_pred = max(0, float(pred_values[semana-1, 0]))  # Canal 0: vendas
                            faturamento_pred = max(0, float(pred_values[semana-1, 1]))  # Canal 1: faturamento
                            
                            resultados.append({
                                'produto_cat': produto,
                                'uf_cat': estado,
                                'Semana': semana,
                                'Vendas': round(vendas_pred, 2),
                                'Faturamento': round(faturamento_pred, 2)
                            })
                    else:
                        # Se n√£o conseguir criar dataset, usar zeros
                        for semana in range(1, n_semanas + 1):
                            resultados.append({
                                'produto_cat': produto,
                                'uf_cat': estado,
                                'Semana': semana,
                                'Vendas': 0,
                                'Faturamento': 0
                            })
                            
                except (RuntimeError, ValueError) as e:
                    print(f"Erro espec√≠fico para produto {produto}, estado {estado}: {type(e).__name__}: {e}")
                    # Em caso de erro, usar zeros
                    for semana in range(1, n_semanas + 1):
                        resultados.append({
                            'produto_cat': produto,
                            'uf_cat': estado,
                            'Semana': semana,
                            'Vendas': 0,
                            'Faturamento': 0
                        })
                except Exception as e:
                    print(f"Erro inesperado para produto {produto}, estado {estado}: {e}")
                    # Em caso de erro, usar zeros
                    for semana in range(1, n_semanas + 1):
                        resultados.append({
                            'produto_cat': produto,
                            'uf_cat': estado,
                            'Semana': semana,
                            'Vendas': 0,
                            'Faturamento': 0
                        })
        
        # Salvar resultados
        df_previsoes = pd.DataFrame(resultados)
        df_previsoes.to_csv(arquivo_saida, index=False)
        
        print(f"‚úÖ Previs√µes salvas em: {arquivo_saida}")
        print(f"üìä Total de registros: {len(df_previsoes)}")
        
        # Estat√≠sticas resumidas
        if len(df_previsoes) > 0:
            print(f"üìà Vendas - M√©dia: {df_previsoes['Vendas'].mean():.2f}, Max: {df_previsoes['Vendas'].max():.2f}")
            print(f"üí∞ Faturamento - M√©dia: {df_previsoes['Faturamento'].mean():.2f}, Max: {df_previsoes['Faturamento'].max():.2f}")
        
        return df_previsoes
    
    # Definir listas
    LISTA_PRODUTOS = list(range(1, 151))  # 1 a 150
    LISTA_ESTADOS = [1, 2, 3, 4, 5, 6]   # 1 a 6
    
    # Executar gera√ß√£o de previs√µes
    try:
        df_automatizado = gerar_previsoes_automatizadas(
            trainer_modelo=trainer,
            preprocessador_treinado=trained_tsp,
            lista_produtos=LISTA_PRODUTOS,
            lista_estados=LISTA_ESTADOS,
            arquivo_saida='previsoes_26_semanas.csv'
        )
        
        print(f"\nüìã Primeiras 10 linhas:")
        print(df_automatizado.head(10))
        
        print(f"\nüìã √öltimas 10 linhas:")
        print(df_automatizado.tail(10))
        
    except Exception as e:
        print(f"‚ùå ERRO na infer√™ncia automatizada: {e}")

# =================== FIM DO C√ìDIGO TEMPOR√ÅRIO ===================
